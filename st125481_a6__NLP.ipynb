{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf\n",
        "!pip install -U langchain-community\n",
        "!pip install faiss-cpu\n",
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xA8VR1V2nrci",
        "outputId": "c822d4bc-88e6-4576-b8d8-1589e2c72fc4"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.4.0)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.19)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.41 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.44)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.20 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.20)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.39)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.13)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.0.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.8.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.13)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.20->langchain-community) (0.3.6)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.20->langchain-community) (2.10.6)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.20->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.20->langchain-community) (2.27.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.48.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (1.14.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (0.28.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import faiss\n",
        "import numpy as np\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.chains import RetrievalQA\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "\n",
        "# Set Hugging Face API Token\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"  # Replace with your actual token\n"
      ],
      "metadata": {
        "id": "gaUX3P3enrY6"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Hugging Face API Token\n",
        "hf_token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
        "if hf_token is None:\n",
        "    raise ValueError(\"HUGGINGFACEHUB_API_TOKEN is not set. Please set it in your environment variables.\")\n"
      ],
      "metadata": {
        "id": "Ihzjw3qV2o6q"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Initialize Hugging Face LLM\n",
        "llm = HuggingFaceHub(\n",
        "    repo_id=\"google/flan-t5-large\",\n",
        "    huggingfacehub_api_token=hf_token,\n",
        "    model_kwargs={\"temperature\": 0.7, \"max_length\": 512},\n",
        "    task=\"text2text-generation\"  # Specify the task explicitly\n",
        ")"
      ],
      "metadata": {
        "id": "EoXBVMpQoH5g"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define FAISS index file path\n",
        "INDEX_PATH = \"faiss_index.bin\"\n",
        "\n",
        "# Check if FAISS index exists and load it if available\n",
        "if os.path.exists(INDEX_PATH):\n",
        "    index = faiss.read_index(INDEX_PATH)\n",
        "    print(\"FAISS index loaded from disk.\")\n",
        "else:\n",
        "    print(\"FAISS index not found. Rebuilding...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGu5sXwc3F6r",
        "outputId": "09a28334-ed43-4557-c817-b752209dffff"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS index loaded from disk.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_files = [\n",
        "    \"/content/RIDA FATMA Resume.pdf\"\n",
        "]\n",
        "\n",
        "documents = []\n",
        "for pdf_file in pdf_files:\n",
        "    loader = PyPDFLoader(pdf_file)\n",
        "    documents.extend(loader.load())\n",
        "\n",
        "# Split documents into chunks\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "text_chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "# Extract text content\n",
        "texts = [doc.page_content for doc in text_chunks]\n"
      ],
      "metadata": {
        "id": "3AVMz4l2nrWp"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "embeddings = embedding_model.encode(texts, convert_to_tensor=False)\n",
        "embedding_matrix = np.array(embeddings).astype(\"float32\")\n",
        "\n",
        "# Initialize FAISS index\n",
        "index = faiss.IndexFlatL2(embedding_matrix.shape[1])\n",
        "index.add(embedding_matrix)\n",
        "\n",
        "# Save FAISS index to disk\n",
        "faiss.write_index(index, \"faiss_index.bin\")\n"
      ],
      "metadata": {
        "id": "AGy7RA3YnrTo"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create FAISS vector store\n",
        "docstore = InMemoryStore()\n",
        "index_to_docstore_id = {}\n",
        "\n",
        "document_objects = []\n",
        "for i, doc in enumerate(text_chunks):\n",
        "    doc_object = Document(page_content=doc.page_content, metadata=doc.metadata)\n",
        "    document_objects.append(doc_object)\n",
        "    index_to_docstore_id[i] = str(i)\n",
        "\n",
        "docstore.mset([(str(i), doc) for i, doc in enumerate(document_objects)])\n",
        "\n",
        "vector_store = FAISS(\n",
        "    embedding_function=embedding_model.encode,\n",
        "    index=index,\n",
        "    docstore=docstore,\n",
        "    index_to_docstore_id=index_to_docstore_id\n",
        ")\n",
        "\n",
        "\n",
        "# Override `docstore.search` with `mget()`\n",
        "def docstore_get(doc_id):\n",
        "    docs = docstore.mget([doc_id])\n",
        "    return docs[0] if docs else None\n",
        "\n",
        "vector_store.docstore.search = docstore_get # Overriding the search method of the docstore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-V5ERgWp1Ut",
        "outputId": "41715ee5-b7e0-4569-b2b7-51723064d371"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.vectorstores.faiss:`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup Retriever\n",
        "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
        "\n",
        "# Define the structured prompt\n",
        "prompt_template = \"\"\"\n",
        "You are an intelligent assistant with expertise in providing information about Ponkrit Kaewsawee.\n",
        "Your answers should be accurate, concise, and strictly based on the provided documents.\n",
        "If the information is not available, kindly respond that you lack sufficient data.\n",
        "\n",
        "Question: {question}\n",
        "Answer:\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "jWak-JvinrNr"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "pexqFw_8nrLT"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to ask chatbot questions\n",
        "def ask_chatbot(question):\n",
        "    retrieved_docs = retriever.get_relevant_documents(question)\n",
        "\n",
        "    if not retrieved_docs:\n",
        "        return \"No relevant information found.\", []\n",
        "\n",
        "    response = qa_chain.invoke({\"query\": question})\n",
        "    return response[\"result\"], response[\"source_documents\"]\n",
        "\n",
        "    if not retrieved_docs:\n",
        "        return \"No relevant information found.\", []\n",
        "\n",
        "    response = qa_chain.invoke({\"query\": question})\n",
        "    return response[\"result\"], response[\"source_documents\"]"
      ],
      "metadata": {
        "id": "wQecGIBanrIq"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tnA77kdLnrDB"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "wvoY9eS2nrAR"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.chains import RetrievalQA\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_core.documents import Document\n",
        "from langchain.llms import HuggingFaceHub\n",
        "\n",
        "\n",
        "# Set the Hugging Face API Token as an environment variable\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\""
      ],
      "metadata": {
        "id": "UFCiS1gOnq9Q"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Hugging Face LLM\n",
        "hf_llm = HuggingFaceHub(\n",
        "    repo_id=\"google/flan-t5-large\",\n",
        "    huggingfacehub_api_token=hf_token,\n",
        "    model_kwargs={\"temperature\": 0.7, \"max_length\": 512}\n",
        ")\n",
        "\n",
        "# Initialize another Hugging Face LLM (e.g., GPT-2)\n",
        "hf_llm_alternate = HuggingFaceHub(\n",
        "    repo_id=\"gpt2\",\n",
        "    huggingfacehub_api_token=hf_token,\n",
        "    model_kwargs={\"temperature\": 0.7, \"max_length\": 512}\n",
        ")\n"
      ],
      "metadata": {
        "id": "_ta_NtsPnq6o"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Groq Cloud Llama LLM\n",
        "# Replace 'your_groq_api_key' and 'your_groq_endpoint' with actual values\n",
        "# groq_llm = GroqLlama(api_key=\"your_groq_api_key\", endpoint=\"your_groq_endpoint\")\n",
        "\n",
        "# Define FAISS index file path\n",
        "INDEX_PATH = \"faiss_index.bin\"\n",
        "\n",
        "# Check if FAISS index exists and load it if available\n",
        "if os.path.exists(INDEX_PATH):\n",
        "    index = faiss.read_index(INDEX_PATH)\n",
        "    print(\"FAISS index loaded from disk.\")\n",
        "else:\n",
        "    print(\"FAISS index not found. Rebuilding...\")\n",
        "\n",
        "    # Load Personal Documents\n",
        "    pdf_files = [\n",
        "        \"/content/RIDA FATMA Resume.pdf\"\n",
        "    ]\n",
        "\n",
        "    documents = []\n",
        "    for pdf_file in pdf_files:\n",
        "        loader = PyPDFLoader(pdf_file)\n",
        "        documents.extend(loader.load())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7RxNW6Inq3u",
        "outputId": "69a126a9-81b4-4a10-994a-350985186327"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS index loaded from disk.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    # Split documents into chunks\n",
        "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "    text_chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "    # Extract text content from chunks\n",
        "    texts = [doc.page_content for doc in text_chunks]\n",
        "\n",
        "    # Convert text to embeddings using SentenceTransformer\n",
        "    embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "    embeddings = embedding_model.encode(texts, convert_to_tensor=False)\n",
        "\n",
        "    # Convert embeddings to numpy array for FAISS\n",
        "    embedding_matrix = np.array(embeddings).astype(\"float32\")\n",
        "\n",
        "    # Initialize FAISS index\n",
        "    index = faiss.IndexFlatL2(embedding_matrix.shape[1])\n",
        "    index.add(embedding_matrix)\n",
        "\n",
        "    # Save FAISS index to disk\n",
        "    faiss.write_index(index, INDEX_PATH)\n",
        "    print(\"FAISS index saved to disk.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLc8bbNx4tS-",
        "outputId": "15077eb5-d8ce-4ce6-97db-1b3ff566f06b"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS index saved to disk.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create FAISS vector store\n",
        "docstore = InMemoryStore()\n",
        "index_to_docstore_id = {}\n",
        "\n",
        "document_objects = []\n",
        "for i, doc in enumerate(text_chunks):\n",
        "    doc_object = Document(page_content=doc.page_content, metadata=doc.metadata)\n",
        "    document_objects.append(doc_object)\n",
        "    index_to_docstore_id[i] = str(i)\n",
        "\n",
        "docstore.mset([(str(i), doc) for i, doc in enumerate(document_objects)])\n",
        "\n",
        "vector_store = FAISS(\n",
        "    embedding_function=embedding_model.encode,\n",
        "    index=index,\n",
        "    docstore=docstore,\n",
        "    index_to_docstore_id=index_to_docstore_id\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKSRDEu670zm",
        "outputId": "4b5aaf26-fb4e-4f6b-d310-63716d547242"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.vectorstores.faiss:`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Override `docstore.search` with `mget()`\n",
        "def docstore_get(doc_id):\n",
        "    docs = docstore.mget([doc_id])\n",
        "    return docs[0] if docs else None\n",
        "\n",
        "vector_store.docstore.search = docstore_get\n",
        "\n",
        "# Setup Retriever\n",
        "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n"
      ],
      "metadata": {
        "id": "XVq-Rbtm702B"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"\n",
        "You are an AI assistant designed to provide information about Rida Fatma, based exclusively on the details provided in the document below. If you are unable to find the answer to a question within the document, please respond with \"The information you're seeking is not available in the provided document.\"\n",
        "\n",
        "Document Content:\n",
        "{context}\n",
        "\n",
        "User's Question:\n",
        "{question}\n",
        "\n",
        "Your Response:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "kOOeM5ze8Yfj"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up LangChain RetrievalQA chain for each model\n",
        "qa_chain_hf = RetrievalQA.from_chain_type(\n",
        "    llm=hf_llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "qa_chain_hf_alternate = RetrievalQA.from_chain_type(\n",
        "    llm=hf_llm_alternate,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "mzoSJJps705D"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to ask chatbot questions for different models\n",
        "def ask_chatbot(question, model=\"hf\"):\n",
        "    if model == \"hf\":\n",
        "        qa_chain = qa_chain_hf\n",
        "    elif model == \"hf_alternate\":\n",
        "        qa_chain = qa_chain_hf_alternate\n",
        "    elif model == \"groq\":\n",
        "        # qa_chain = qa_chain_groq\n",
        "        pass\n",
        "    else:\n",
        "        return \"Invalid model specified.\", []\n",
        "\n",
        "    retrieved_docs = retriever.get_relevant_documents(question)\n",
        "\n",
        "    if not retrieved_docs:\n",
        "        return \"No relevant information found.\", []\n",
        "\n",
        "    response = qa_chain.invoke({\"query\": question})\n",
        "    return response[\"result\"], response[\"source_documents\"]\n"
      ],
      "metadata": {
        "id": "Wj8USkdg8Ajp"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of reference documents\n",
        "reference_documents = pdf_files\n",
        "print(\"Reference Documents:\", reference_documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKkBmMIM8AmJ",
        "outputId": "20224849-dab2-4514-c437-7c0fab096c91"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reference Documents: ['/content/RIDA FATMA Resume.pdf']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sVUjczQo8Apj"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analysis and Problem Solving\n",
        "\n",
        "# Models Used\n",
        "retriever_model = \"SentenceTransformer - all-MiniLM-L6-v2\"\n",
        "generator_model = \"Hugging Face - google/flan-t5-large\"\n",
        "\n",
        "print(\"Document Retrieval Model:\", retriever_model)  # Changed to 'Document Retrieval Model'\n",
        "print(\"Response Generation Model:\", generator_model) # Changed to 'Response Generation Model'\n",
        "\n",
        "# Challenges in Providing Relevant Information\n",
        "# Potential issues:\n",
        "# 1. Limitations in Semantic Understanding: The retriever model might struggle to capture the nuances of the query and retrieve documents with only superficial similarity.\n",
        "# 2. Prompt Engineering: The generator model might produce irrelevant responses if the prompt lacks sufficient guidance or structure to connect the query with the retrieved context.\n",
        "\n",
        "# Strategies for Improvement\n",
        "# 1. Fine-tuning for Specificity: Consider fine-tuning the retriever model on a dataset of questions and relevant documents closely aligned with your personal content.\n",
        "# 2. Prompt Optimization: Experiment with different prompt structures and instructions to provide the generator model with clearer cues and expectations.\n",
        "# 3. Incorporating Feedback: Introduce a feedback loop to iteratively refine the retrieval and generation process based on user interactions and performance evaluations."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oti2sc9s8nuB",
        "outputId": "fc5c816a-7092-4d0b-b82b-cd4d3b728651"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document Retrieval Model: SentenceTransformer - all-MiniLM-L6-v2\n",
            "Response Generation Model: Hugging Face - google/flan-t5-large\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UMywJ-lS8nwQ"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHhWmyWD8nyi",
        "outputId": "a7c9d3e0-2b5f-443b-9f78-3c91a478b5c6"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.43.2)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.1.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.25.6)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.30.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.23.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import faiss\n",
        "import numpy as np\n",
        "import streamlit as st\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.chains import RetrievalQA\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_core.documents import Document\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "# Set page title and favicon\n",
        "st.set_page_config(page_title=\"Rida Fatma's Resume Chatbot\", page_icon=\":robot_face:\")\n",
        "\n",
        "\n",
        "# Set the Hugging Face API Token as an environment variable\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\" # Replace with your actual token\n",
        "# Load Hugging Face API Token\n",
        "hf_token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
        "if hf_token is None:\n",
        "    raise ValueError(\"HUGGINGFACEHUB_API_TOKEN is not set. Please set it in your environment variables.\")\n",
        "\n",
        "# Initialize Hugging Face LLM\n",
        "hf_llm = HuggingFaceHub(\n",
        "    repo_id=\"google/flan-t5-large\",\n",
        "    huggingfacehub_api_token=hf_token,\n",
        "    model_kwargs={\"temperature\": 0.7, \"max_length\": 512}\n",
        ")\n",
        "\n",
        "# ... (Rest of the code for document loading, embedding,\n",
        "# and RetrievalQA setup remains the same as before) ...\n",
        "\n",
        "# Streamlit UI\n",
        "st.title(\"Rida Fatma's Resume Chatbot :robot_face:\")\n",
        "st.write(\"Ask me anything about Rida Fatma's resume!\")\n",
        "\n",
        "user_question = st.text_input(\"Enter your question:\")\n",
        "\n",
        "if user_question:\n",
        "    with st.spinner(\"Thinking...\"):\n",
        "        answer, source_documents = ask_chatbot(user_question)\n",
        "        st.write(\"**Answer:**\", answer)\n",
        "\n",
        "        # Display source documents (optional)\n",
        "        # st.write(\"**Source Documents:**\")\n",
        "        # for doc in source_documents:\n",
        "        #     st.write(doc.page_content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIXVAFqs8n1r",
        "outputId": "8e2b4608-b29e-4641-9ba4-132a64cd055b"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-03-16 12:46:44.635 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-16 12:46:44.913 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-16 12:46:44.915 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-16 12:46:44.915 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-16 12:46:44.916 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-16 12:46:44.917 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-16 12:46:44.918 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-16 12:46:44.918 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-16 12:46:44.919 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-16 12:46:44.920 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-16 12:46:44.922 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-16 12:46:44.922 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-16 12:46:44.923 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VuwODtdd71BZ"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UIrDKd_x71Ew"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WH_Xt_ig9DKH"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4NRecNgY9DNJ"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Is_bma_-9DPy"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DrmdRTte9DSK"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4M2oH4-hgAHp"
      },
      "execution_count": 90,
      "outputs": []
    }
  ]
}